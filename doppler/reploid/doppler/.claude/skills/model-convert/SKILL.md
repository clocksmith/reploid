---
name: model-convert
description: Convert GGUF or SafeTensors models to RDRR format and test them in DOPPLER. Use when the user wants to add a new model, convert weights, or verify model loading.
---

# Model Conversion Skill

Convert models to DOPPLER's RDRR format and verify they work.

## Conversion CLI

```bash
# From HuggingFace directory with quantization
npx tsx doppler/reploid/doppler/tools/convert-cli.ts \
  ~/models/Llama-3.2-1B \
  doppler/reploid/doppler/models/llama-1b \
  --quantize q4_k_m

# From GGUF file
npx tsx doppler/reploid/doppler/tools/convert-cli.ts \
  ~/models/model.gguf \
  doppler/reploid/doppler/models/model-name

# Multimodal to text-only (strips vision tower)
npx tsx doppler/reploid/doppler/tools/convert-cli.ts \
  ~/models/gemma-3-4b-it \
  doppler/reploid/doppler/models/gemma-4b-text \
  --text-only --quantize q4_k_m

# Create tiny test fixture
npx tsx doppler/reploid/doppler/tools/convert-cli.ts --test ./test-model
```

## Options

| Flag | Description |
|------|-------------|
| `--quantize q4_k_m` | Quantize to Q4_K_M (recommended) |
| `--quantize f16` | Keep as FP16 |
| `--quantize-embeddings` | Also quantize embedding table |
| `--shard-size <mb>` | Shard size in MB (default: 64) |
| `--model-id <id>` | Override model ID in manifest |
| `--text-only` | Extract only text model from multimodal |
| `--fast` | Pre-load shards (faster, more RAM) |
| `--verbose` | Show detailed progress |

## Testing the Converted Model

```bash
# Start dev server
npx tsx doppler/reploid/doppler/serve.ts &

# Run E2E test with browser UI
npx tsx doppler/reploid/doppler/tests/test-runner.ts <model-name> --direct --headed

# Or use Playwright
npx playwright test doppler/reploid/doppler/tests/gemma-e2e.spec.ts --headed
```

## Workflow

1. **Locate source model**
   - HuggingFace cache: `~/.cache/huggingface/hub/models--<org>--<model>/snapshots/<hash>/`
   - Local GGUF: Any `.gguf` file
   - Local SafeTensors: Directory with `*.safetensors` files

2. **Convert with appropriate options**
   - Use `--quantize q4_k_m` for smaller size
   - Use `--text-only` for multimodal models (Gemma 3, PaliGemma)
   - Use `--fast` if you have enough RAM

3. **Verify conversion output**
   - Check `manifest.json` in output directory
   - Verify tensor count and shard count
   - Check model config is correctly inferred

4. **Test in browser**
   - Start server, run E2E test
   - Check for inference errors in console
   - If issues, use `doppler-debug` skill

## Supported Input Formats

| Format | Extension | Source |
|--------|-----------|--------|
| GGUF | `.gguf` | llama.cpp |
| SafeTensors | `.safetensors` | HuggingFace |
| HF Directory | folder | HuggingFace Hub |
| Index JSON | `model.safetensors.index.json` | Sharded HF models |

## Output Structure

```
models/<model-name>/
  manifest.json       # Model metadata, tensor index
  shard_00000.bin     # Weight shards
  shard_00001.bin
  ...
```

## Common Issues

| Issue | Cause | Fix |
|-------|-------|-----|
| "Unknown architecture" | Model type not recognized | Check MODEL_SUPPORT.md for supported archs |
| Config values missing | HF config incomplete | Converter infers from tensor shapes |
| Large output size | No quantization | Add `--quantize q4_k_m` |
| Missing tensors | Multimodal model | Add `--text-only` for text-only extraction |

## Reference

- Model support matrix: `doppler/reploid/doppler/docs/plans/MODEL_SUPPORT.md`
- RDRR format spec: `doppler/reploid/doppler/docs/spec/RDRR_FORMAT.md`
- Troubleshooting: `doppler/reploid/doppler/docs/DOPPLER-TROUBLESHOOTING.md`

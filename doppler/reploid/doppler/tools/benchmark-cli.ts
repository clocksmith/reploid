#!/usr/bin/env node
/**
 * Benchmark CLI - Run DOPPLER benchmarks from command line
 *
 * Uses Playwright to run benchmarks in a real browser (WebGPU required).
 *
 * Usage:
 *   npx tsx tools/benchmark-cli.ts [options]
 *
 * Examples:
 *   npx tsx tools/benchmark-cli.ts --model gemma-1b
 *   npx tsx tools/benchmark-cli.ts --model gemma-1b --runs 5 --prompt medium
 *   npx tsx tools/benchmark-cli.ts --model gemma-1b --suite full --output results.json
 */

import { chromium, type Browser, type Page } from 'playwright';
import { writeFile, mkdir, readFile } from 'fs/promises';
import { resolve, dirname } from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// ============================================================================
// Types
// ============================================================================

interface BenchmarkOptions {
  model: string;
  baseUrl: string;
  prompt: 'short' | 'medium' | 'long';
  maxTokens: number;
  warmupRuns: number;
  timedRuns: number;
  temperature: number;
  suite: 'quick' | 'full' | 'pipeline' | 'system' | 'compare';
  output: string | null;
  html: string | null; // Path to HTML report output
  headed: boolean;
  timeout: number;
  retries: number;
  verbose: boolean;
  quiet: boolean;
  compare: string | null; // Path to baseline file for comparison
  help: boolean;
}

// ============================================================================
// Argument Parsing
// ============================================================================

function parseArgs(argv: string[]): BenchmarkOptions {
  const opts: BenchmarkOptions = {
    model: 'gemma3-1b-q4',
    baseUrl: 'http://localhost:8080/doppler',
    prompt: 'medium',
    maxTokens: 128,
    warmupRuns: 2,
    timedRuns: 3,
    temperature: 0,
    suite: 'pipeline',
    output: null,
    html: null,
    headed: false,
    timeout: 300000, // 5 minutes
    retries: 2,
    verbose: false,
    quiet: false,
    compare: null,
    help: false,
  };

  const tokens = [...argv];
  while (tokens.length) {
    const arg = tokens.shift()!;
    switch (arg) {
      case '--help':
      case '-h':
        opts.help = true;
        break;
      case '--model':
      case '-m':
        opts.model = tokens.shift() || opts.model;
        break;
      case '--base-url':
      case '-u':
        opts.baseUrl = tokens.shift() || opts.baseUrl;
        break;
      case '--prompt':
      case '-p':
        opts.prompt = (tokens.shift() || opts.prompt) as 'short' | 'medium' | 'long';
        break;
      case '--max-tokens':
      case '-t':
        opts.maxTokens = parseInt(tokens.shift() || '128', 10);
        break;
      case '--warmup':
      case '-w':
        opts.warmupRuns = parseInt(tokens.shift() || '2', 10);
        break;
      case '--runs':
      case '-r':
        opts.timedRuns = parseInt(tokens.shift() || '3', 10);
        break;
      case '--temperature':
        opts.temperature = parseFloat(tokens.shift() || '0');
        break;
      case '--suite':
      case '-s':
        opts.suite = (tokens.shift() || opts.suite) as 'quick' | 'full' | 'pipeline' | 'system' | 'compare';
        break;
      case '--output':
      case '-o':
        opts.output = tokens.shift() || null;
        break;
      case '--html':
        opts.html = tokens.shift() || null;
        break;
      case '--headed':
        opts.headed = true;
        break;
      case '--timeout':
        opts.timeout = parseInt(tokens.shift() || '300000', 10);
        break;
      case '--retries':
        opts.retries = parseInt(tokens.shift() || '2', 10);
        break;
      case '--verbose':
      case '-v':
        opts.verbose = true;
        break;
      case '--quiet':
      case '-q':
        opts.quiet = true;
        break;
      case '--compare':
      case '-c':
        opts.compare = tokens.shift() || null;
        break;
      default:
        // Positional: treat as model name
        if (!arg.startsWith('-')) {
          opts.model = arg;
        }
        break;
    }
  }

  return opts;
}

function printHelp(): void {
  console.log(`
DOPPLER Benchmark CLI - Run performance benchmarks

Usage:
  npx tsx tools/benchmark-cli.ts [model] [options]

Arguments:
  model                Model name or path (default: gemma-1b)

Options:
  --model, -m <name>   Model name in models/ directory
  --base-url, -u <url> Server base URL (default: http://localhost:8080)
  --prompt, -p <size>  Prompt size: short, medium, long (default: medium)
  --max-tokens, -t <n> Max tokens to generate (default: 128)
  --warmup, -w <n>     Warmup runs (default: 2)
  --runs, -r <n>       Timed runs (default: 3)
  --temperature <n>    Sampling temperature, 0=greedy (default: 0)
  --suite, -s <type>   Suite: quick, full, pipeline, system (default: pipeline)
  --output, -o <file>  Output JSON file path
  --html <file>        Custom HTML report path (auto-generated by default)
  --headed             Show browser window
  --timeout <ms>       Timeout in milliseconds (default: 300000)
  --retries <n>        Number of retries on failure (default: 2)
  --verbose, -v        Show all browser console logs
  --quiet, -q          Suppress JSON output to stdout
  --compare, -c <file> Compare results against baseline JSON file
  --help, -h           Show this help

Suites:
  quick     Single short benchmark (fast validation)
  pipeline  Pipeline benchmark with configured prompt
  full      All prompt sizes (short, medium, long)
  system    System benchmark (download/storage performance)

Examples:
  # Quick validation
  npx tsx tools/benchmark-cli.ts gemma-1b --suite quick

  # Standard benchmark
  npx tsx tools/benchmark-cli.ts gemma-1b --runs 5 --prompt medium

  # Full suite with output
  npx tsx tools/benchmark-cli.ts gemma-1b --suite full -o results.json

  # System benchmark
  npx tsx tools/benchmark-cli.ts gemma-1b --suite system

  # Compare against baseline
  npx tsx tools/benchmark-cli.ts gemma-1b --compare baseline.json

  # Verbose mode (show all browser logs)
  npx tsx tools/benchmark-cli.ts gemma-1b --verbose

  # Quiet mode (no JSON to stdout)
  npx tsx tools/benchmark-cli.ts gemma-1b --quiet -o results.json

  # Generate HTML report with charts
  npx tsx tools/benchmark-cli.ts gemma-1b --html report.html

  # Compare with baseline and generate HTML report
  npx tsx tools/benchmark-cli.ts gemma-1b --compare baseline.json --html report.html

Prerequisites:
  1. Start dev server: npm run dev (in doppler/reploid)
  2. Ensure model exists in models/ directory
`);
}

// ============================================================================
// Browser Benchmark Runner
// ============================================================================

async function runBenchmarkInBrowser(opts: BenchmarkOptions): Promise<any> {
  console.log(`\n${'─'.repeat(60)}`);
  console.log('DOPPLER Benchmark');
  console.log(`${'─'.repeat(60)}`);
  console.log(`Model:      ${opts.model}`);
  console.log(`Suite:      ${opts.suite}`);
  console.log(`Prompt:     ${opts.prompt}`);
  console.log(`Max tokens: ${opts.maxTokens}`);
  console.log(`Warmup:     ${opts.warmupRuns}`);
  console.log(`Runs:       ${opts.timedRuns}`);
  console.log(`Retries:    ${opts.retries}`);
  console.log(`${'─'.repeat(60)}\n`);

  let lastError: Error | null = null;

  for (let attempt = 0; attempt <= opts.retries; attempt++) {
    if (attempt > 0) {
      const delay = Math.min(1000 * Math.pow(2, attempt - 1), 10000); // Exponential backoff, max 10s
      console.log(`\nRetrying in ${delay}ms... (attempt ${attempt + 1}/${opts.retries + 1})`);
      await new Promise((r) => setTimeout(r, delay));
    }

    // Use persistent context so OPFS cache survives between runs
    const userDataDir = resolve(__dirname, '../.benchmark-cache');
    const context = await chromium.launchPersistentContext(userDataDir, {
      headless: !opts.headed,
      devtools: opts.headed, // Open DevTools console in headed mode
      args: ['--enable-unsafe-webgpu', '--enable-features=Vulkan', '--auto-open-devtools-for-tabs'],
    });
    const page = context.pages()[0] || await context.newPage();

    // Capture console logs with expanded filtering
    const relevantTags = ['[Benchmark]', '[Pipeline]', '[Loader]', '[DopplerLoader]', '[GPU]', '[Kernel]', 'ERROR', 'WARN', 'error', 'Error'];
    page.on('console', (msg) => {
      const text = msg.text();
      const isRelevant = relevantTags.some((tag) => text.includes(tag));
      if (opts.verbose || isRelevant) {
        console.log(`[browser] ${text}`);
      }
    });

    // Also capture page errors
    page.on('pageerror', (err) => {
      console.error(`[browser error] ${err.message}`);
    });

    try {
      // Navigate to demo page
      console.log('Opening browser...');
      await page.goto(opts.baseUrl, { timeout: 30000 });

      // Wait for WebGPU to be ready (better than hardcoded timeout)
      console.log('Waiting for WebGPU...');
      await page.waitForFunction(
        () => {
          return typeof navigator !== 'undefined' && 'gpu' in navigator;
        },
        { timeout: 10000 }
      );

      // Additional wait for page scripts to initialize
      await page.waitForFunction(
        () => {
          // Check if our benchmark module is loadable
          return typeof (window as any).dopplerReady === 'undefined' || (window as any).dopplerReady === true;
        },
        { timeout: 5000 }
      ).catch(() => {
        // If dopplerReady flag doesn't exist, just wait a short time
      });

      // Brief additional wait for any async initialization
      await page.waitForTimeout(500);

      // Build the benchmark script with progress reporting
      const modelPath = `${opts.baseUrl}/models/${opts.model}`;
      const script = buildBenchmarkScript(opts, modelPath);

      console.log('Running benchmark...');
      const startTime = Date.now();

      // Run with timeout
      const result = await Promise.race([
        page.evaluate(script),
        new Promise((_, reject) =>
          setTimeout(() => reject(new Error('Benchmark timeout')), opts.timeout)
        ),
      ]);

      const elapsed = ((Date.now() - startTime) / 1000).toFixed(1);
      console.log(`\nBenchmark complete! (${elapsed}s)`);

      // Keep browser open for inspection in headed mode
      if (opts.headed) {
        console.log('Keeping browser open for 30s (headed mode)...');
        await page.waitForTimeout(30000);
      }

      await context.close();
      return result;
    } catch (err) {
      lastError = err as Error;
      console.error(`\nAttempt ${attempt + 1} failed:`, lastError.message);
      await context.close();

      // Don't retry on certain errors
      if (lastError.message.includes('timeout') && attempt === opts.retries) {
        break;
      }
    }
  }

  throw lastError || new Error('Benchmark failed after all retries');
}

function buildBenchmarkScript(opts: BenchmarkOptions, modelPath: string): string {
  const config = JSON.stringify({
    modelPath,
    promptName: opts.prompt,
    maxNewTokens: opts.maxTokens,
    warmupRuns: opts.warmupRuns,
    timedRuns: opts.timedRuns,
    sampling: {
      temperature: opts.temperature,
      topK: 1,
      topP: 1,
    },
    debug: true, // Enable debug logging for progress
  });

  // Progress reporting wrapper
  const progressWrapper = `
    const progress = (phase, current, total) => {
      console.log('[Benchmark] ' + phase + ': ' + current + '/' + total);
    };
  `;

  switch (opts.suite) {
    case 'quick':
      return `
        (async () => {
          ${progressWrapper}
          progress('Loading', 1, 1);
          const { runQuickBenchmark } = await import('./tests/benchmark/index.js');
          progress('Running quick benchmark', 1, 1);
          return await runQuickBenchmark('${modelPath}');
        })()
      `;

    case 'full':
      return `
        (async () => {
          ${progressWrapper}
          const { runFullBenchmark } = await import('./tests/benchmark/index.js');
          progress('Running full benchmark', 1, 3);
          const results = [];
          for (const [i, promptName] of ['short', 'medium', 'long'].entries()) {
            progress('Prompt: ' + promptName, i + 1, 3);
            const { PipelineBenchmark } = await import('./tests/benchmark/index.js');
            const harness = new PipelineBenchmark({
              modelPath: '${modelPath}',
              promptName,
              maxNewTokens: 128,
              warmupRuns: 2,
              timedRuns: 3,
            });
            results.push(await harness.run());
          }
          return results;
        })()
      `;

    case 'system':
      return `
        (async () => {
          ${progressWrapper}
          progress('Running system benchmark', 1, 1);
          const { runSystemBenchmark } = await import('./tests/benchmark/index.js');
          return await runSystemBenchmark('${modelPath}');
        })()
      `;

    case 'pipeline':
    default:
      return `
        (async () => {
          ${progressWrapper}
          progress('Loading model', 1, 1);
          const { PipelineBenchmark } = await import('./tests/benchmark/index.js');
          const config = ${config};
          const harness = new PipelineBenchmark(config);
          progress('Running benchmark', 1, 1);
          const result = await harness.run();
          progress('Complete', 1, 1);
          return result;
        })()
      `;
  }
}

// ============================================================================
// Result File Naming
// ============================================================================

function generateResultFilename(result: any): string {
  const suite = result.suite || 'pipeline';
  const model = result.model?.modelName || result.model?.modelId || 'unknown';
  const modelSlug = model.replace(/[^a-zA-Z0-9-_]/g, '-').toLowerCase();

  // Include GPU identifier for hardware-specific comparisons
  const gpu = result.env?.gpu?.description || result.env?.gpu?.device || '';
  const gpuSlug = gpu
    .replace(/[^a-zA-Z0-9-_]/g, '-')
    .replace(/-+/g, '-')
    .replace(/^-|-$/g, '')
    .toLowerCase()
    .slice(0, 30); // Limit length

  const timestamp = new Date().toISOString().replace(/[:.]/g, '-');

  if (gpuSlug) {
    return `${suite}_${modelSlug}_${gpuSlug}_${timestamp}.json`;
  }
  return `${suite}_${modelSlug}_${timestamp}.json`;
}

// ============================================================================
// Comparison Utilities
// ============================================================================

interface ComparisonResult {
  metric: string;
  baseline: number;
  current: number;
  delta: number;
  deltaPercent: number;
  improved: boolean;
}

function compareResults(baseline: any, current: any): ComparisonResult[] {
  const results: ComparisonResult[] = [];
  const bm = baseline.metrics;
  const cm = current.metrics;

  const metrics: Array<{ key: string; name: string; lowerIsBetter: boolean }> = [
    { key: 'ttft_ms', name: 'TTFT', lowerIsBetter: true },
    { key: 'prefill_ms', name: 'Prefill', lowerIsBetter: true },
    { key: 'decode_ms_total', name: 'Decode Total', lowerIsBetter: true },
    { key: 'prefill_tokens_per_sec', name: 'Prefill tok/s', lowerIsBetter: false },
    { key: 'decode_tokens_per_sec', name: 'Decode tok/s', lowerIsBetter: false },
    { key: 'decode_ms_per_token_p50', name: 'Decode P50', lowerIsBetter: true },
    { key: 'decode_ms_per_token_p90', name: 'Decode P90', lowerIsBetter: true },
    { key: 'decode_ms_per_token_p99', name: 'Decode P99', lowerIsBetter: true },
    { key: 'gpu_submit_count_prefill', name: 'GPU Submits (prefill)', lowerIsBetter: true },
    { key: 'gpu_submit_count_decode', name: 'GPU Submits (decode)', lowerIsBetter: true },
  ];

  for (const { key, name, lowerIsBetter } of metrics) {
    const baseVal = bm[key];
    const currVal = cm[key];
    if (baseVal !== undefined && currVal !== undefined && baseVal !== 0) {
      const delta = currVal - baseVal;
      const deltaPercent = (delta / baseVal) * 100;
      const improved = lowerIsBetter ? delta < 0 : delta > 0;
      results.push({
        metric: name,
        baseline: baseVal,
        current: currVal,
        delta,
        deltaPercent,
        improved,
      });
    }
  }

  return results;
}

function formatComparison(comparisons: ComparisonResult[]): string {
  const lines: string[] = [
    '',
    '='.repeat(60),
    'COMPARISON VS BASELINE',
    '='.repeat(60),
    '',
  ];

  for (const c of comparisons) {
    const sign = c.delta >= 0 ? '+' : '';
    const arrow = c.improved ? '\u2193' : c.delta === 0 ? '=' : '\u2191'; // Down arrow good, up arrow bad (for latency)
    const status = c.improved ? 'BETTER' : c.delta === 0 ? 'SAME' : 'WORSE';
    lines.push(
      `${c.metric.padEnd(20)} ${c.baseline.toFixed(1).padStart(10)} -> ${c.current.toFixed(1).padStart(10)}  ${sign}${c.deltaPercent.toFixed(1)}% ${arrow} ${status}`
    );
  }

  // Summary
  const improved = comparisons.filter((c) => c.improved).length;
  const regressed = comparisons.filter((c) => !c.improved && c.delta !== 0).length;
  lines.push('');
  lines.push(`Summary: ${improved} improved, ${regressed} regressed, ${comparisons.length - improved - regressed} unchanged`);

  return lines.join('\n');
}

// ============================================================================
// Statistical Significance (Welch's t-test)
// ============================================================================

interface TTestResult {
  tStatistic: number;
  degreesOfFreedom: number;
  pValue: number;
  significant: boolean; // p < 0.05
  meanA: number;
  meanB: number;
  stdA: number;
  stdB: number;
}

function mean(values: number[]): number {
  return values.reduce((a, b) => a + b, 0) / values.length;
}

function variance(values: number[], m: number): number {
  return values.reduce((sum, v) => sum + (v - m) ** 2, 0) / (values.length - 1);
}

/**
 * Welch's t-test for comparing two samples with unequal variance
 */
function welchTTest(a: number[], b: number[]): TTestResult {
  const n1 = a.length;
  const n2 = b.length;
  const m1 = mean(a);
  const m2 = mean(b);
  const v1 = variance(a, m1);
  const v2 = variance(b, m2);

  const se1 = v1 / n1;
  const se2 = v2 / n2;
  const se = Math.sqrt(se1 + se2);

  const t = (m1 - m2) / se;

  // Welch-Satterthwaite degrees of freedom
  const num = (se1 + se2) ** 2;
  const denom = (se1 ** 2) / (n1 - 1) + (se2 ** 2) / (n2 - 1);
  const df = num / denom;

  // Approximate p-value using t-distribution (two-tailed)
  const pValue = tDistPValue(Math.abs(t), df);

  return {
    tStatistic: t,
    degreesOfFreedom: df,
    pValue,
    significant: pValue < 0.05,
    meanA: m1,
    meanB: m2,
    stdA: Math.sqrt(v1),
    stdB: Math.sqrt(v2),
  };
}

/**
 * Approximate p-value for t-distribution (two-tailed)
 * Uses approximation suitable for df > 1
 */
function tDistPValue(t: number, df: number): number {
  // Using approximation: p ≈ 2 * (1 - Φ(t * sqrt(df/(df-2+t²))))
  // where Φ is standard normal CDF
  const x = t * Math.sqrt(df / (df - 2 + t * t));
  const p = 2 * (1 - normalCDF(x));
  return Math.max(0, Math.min(1, p));
}

/**
 * Standard normal CDF approximation (Abramowitz & Stegun)
 */
function normalCDF(x: number): number {
  const a1 = 0.254829592;
  const a2 = -0.284496736;
  const a3 = 1.421413741;
  const a4 = -1.453152027;
  const a5 = 1.061405429;
  const p = 0.3275911;

  const sign = x < 0 ? -1 : 1;
  x = Math.abs(x) / Math.sqrt(2);

  const t = 1 / (1 + p * x);
  const y = 1 - ((((a5 * t + a4) * t + a3) * t + a2) * t + a1) * t * Math.exp(-x * x);

  return 0.5 * (1 + sign * y);
}

function formatTTestResult(metric: string, result: TTestResult): string {
  const sig = result.significant ? 'SIGNIFICANT' : 'not significant';
  return `${metric}: t=${result.tStatistic.toFixed(2)}, df=${result.degreesOfFreedom.toFixed(1)}, p=${result.pValue.toFixed(4)} (${sig})`;
}

// ============================================================================
// HTML Report Generation (Inline SVG)
// ============================================================================

function generateSVGBarChart(
  data: Array<{ label: string; value: number; color?: string }>,
  width = 400,
  height = 200,
  title = ''
): string {
  const margin = { top: 30, right: 20, bottom: 40, left: 60 };
  const chartWidth = width - margin.left - margin.right;
  const chartHeight = height - margin.top - margin.bottom;

  const maxValue = Math.max(...data.map((d) => d.value)) * 1.1;
  const barWidth = chartWidth / data.length - 10;

  let svg = `<svg width="${width}" height="${height}" xmlns="http://www.w3.org/2000/svg">`;
  svg += `<style>
    .chart-title { font: bold 14px sans-serif; }
    .axis-label { font: 11px sans-serif; fill: #666; }
    .bar-label { font: 10px sans-serif; fill: #333; }
    .grid-line { stroke: #e0e0e0; stroke-width: 1; }
  </style>`;

  // Title
  if (title) {
    svg += `<text x="${width / 2}" y="18" text-anchor="middle" class="chart-title">${title}</text>`;
  }

  // Grid lines
  for (let i = 0; i <= 4; i++) {
    const y = margin.top + (chartHeight * i) / 4;
    svg += `<line x1="${margin.left}" y1="${y}" x2="${width - margin.right}" y2="${y}" class="grid-line"/>`;
    const val = (maxValue * (4 - i)) / 4;
    svg += `<text x="${margin.left - 5}" y="${y + 4}" text-anchor="end" class="axis-label">${val.toFixed(0)}</text>`;
  }

  // Bars
  data.forEach((d, i) => {
    const barHeight = (d.value / maxValue) * chartHeight;
    const x = margin.left + i * (chartWidth / data.length) + 5;
    const y = margin.top + chartHeight - barHeight;
    const color = d.color || '#4a90d9';

    svg += `<rect x="${x}" y="${y}" width="${barWidth}" height="${barHeight}" fill="${color}" rx="2"/>`;
    svg += `<text x="${x + barWidth / 2}" y="${height - margin.bottom + 15}" text-anchor="middle" class="bar-label">${d.label}</text>`;
    svg += `<text x="${x + barWidth / 2}" y="${y - 5}" text-anchor="middle" class="bar-label">${d.value.toFixed(1)}</text>`;
  });

  svg += '</svg>';
  return svg;
}

function generateSVGLineChart(
  data: number[],
  width = 400,
  height = 150,
  title = '',
  yLabel = ''
): string {
  const margin = { top: 30, right: 20, bottom: 30, left: 50 };
  const chartWidth = width - margin.left - margin.right;
  const chartHeight = height - margin.top - margin.bottom;

  const maxValue = Math.max(...data) * 1.1;
  const minValue = Math.min(...data) * 0.9;
  const range = maxValue - minValue;

  let svg = `<svg width="${width}" height="${height}" xmlns="http://www.w3.org/2000/svg">`;
  svg += `<style>
    .chart-title { font: bold 12px sans-serif; }
    .axis-label { font: 10px sans-serif; fill: #666; }
    .line { fill: none; stroke: #4a90d9; stroke-width: 2; }
    .dot { fill: #4a90d9; }
    .grid-line { stroke: #e0e0e0; stroke-width: 1; }
  </style>`;

  // Title
  if (title) {
    svg += `<text x="${width / 2}" y="15" text-anchor="middle" class="chart-title">${title}</text>`;
  }

  // Y-axis label
  if (yLabel) {
    svg += `<text x="12" y="${height / 2}" text-anchor="middle" transform="rotate(-90, 12, ${height / 2})" class="axis-label">${yLabel}</text>`;
  }

  // Grid lines
  for (let i = 0; i <= 3; i++) {
    const y = margin.top + (chartHeight * i) / 3;
    svg += `<line x1="${margin.left}" y1="${y}" x2="${width - margin.right}" y2="${y}" class="grid-line"/>`;
    const val = maxValue - (range * i) / 3;
    svg += `<text x="${margin.left - 5}" y="${y + 4}" text-anchor="end" class="axis-label">${val.toFixed(1)}</text>`;
  }

  // Line path
  const points = data.map((v, i) => {
    const x = margin.left + (i / (data.length - 1)) * chartWidth;
    const y = margin.top + ((maxValue - v) / range) * chartHeight;
    return `${x},${y}`;
  });
  svg += `<polyline points="${points.join(' ')}" class="line"/>`;

  // Dots
  data.forEach((v, i) => {
    const x = margin.left + (i / (data.length - 1)) * chartWidth;
    const y = margin.top + ((maxValue - v) / range) * chartHeight;
    svg += `<circle cx="${x}" cy="${y}" r="3" class="dot"/>`;
  });

  svg += '</svg>';
  return svg;
}

function generateHTMLReport(results: any, baseline?: any): string {
  const isArray = Array.isArray(results);
  const resultList = isArray ? results : [results];
  const firstResult = resultList[0];

  const model = firstResult.model?.modelName || firstResult.model?.modelId || 'Unknown Model';
  const timestamp = new Date().toISOString();
  const env = firstResult.env || {};

  let html = `<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>DOPPLER Benchmark Report - ${model}</title>
  <style>
    * { box-sizing: border-box; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }
    .container { max-width: 1200px; margin: 0 auto; }
    .card { background: white; border-radius: 8px; padding: 20px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
    h1 { color: #333; margin-top: 0; }
    h2 { color: #555; border-bottom: 2px solid #4a90d9; padding-bottom: 8px; }
    h3 { color: #666; }
    .grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; }
    .metric { padding: 15px; background: #f8f9fa; border-radius: 6px; }
    .metric-value { font-size: 28px; font-weight: bold; color: #4a90d9; }
    .metric-label { font-size: 14px; color: #666; }
    .metric-unit { font-size: 14px; color: #999; }
    table { width: 100%; border-collapse: collapse; margin: 10px 0; }
    th, td { padding: 10px; text-align: left; border-bottom: 1px solid #e0e0e0; }
    th { background: #f8f9fa; font-weight: 600; }
    .better { color: #28a745; }
    .worse { color: #dc3545; }
    .chart-container { margin: 20px 0; text-align: center; }
    .env-info { font-size: 13px; color: #666; }
    .timestamp { font-size: 12px; color: #999; }
    .significance { padding: 4px 8px; border-radius: 4px; font-size: 12px; }
    .sig-yes { background: #d4edda; color: #155724; }
    .sig-no { background: #f8f9fa; color: #666; }
  </style>
</head>
<body>
  <div class="container">
    <div class="card">
      <h1>DOPPLER Benchmark Report</h1>
      <p class="timestamp">Generated: ${timestamp}</p>
      <div class="env-info">
        <strong>Model:</strong> ${model} |
        <strong>Browser:</strong> ${env.browser?.name || 'Unknown'} ${env.browser?.version || ''} |
        <strong>GPU:</strong> ${env.gpu?.description || env.gpu?.device || 'Unknown'} |
        <strong>OS:</strong> ${env.os?.name || 'Unknown'}
      </div>
    </div>
`;

  // Summary metrics
  for (const result of resultList) {
    const m = result.metrics;
    const prompt = result.workload?.promptName || 'default';

    html += `
    <div class="card">
      <h2>Results: ${prompt} prompt</h2>
      <div class="grid">
        <div class="metric">
          <div class="metric-value">${m.ttft_ms}<span class="metric-unit">ms</span></div>
          <div class="metric-label">Time to First Token</div>
        </div>
        <div class="metric">
          <div class="metric-value">${m.prefill_tokens_per_sec}<span class="metric-unit">tok/s</span></div>
          <div class="metric-label">Prefill Throughput</div>
        </div>
        <div class="metric">
          <div class="metric-value">${m.decode_tokens_per_sec}<span class="metric-unit">tok/s</span></div>
          <div class="metric-label">Decode Throughput</div>
        </div>
        <div class="metric">
          <div class="metric-value">${m.gpu_submit_count_prefill + m.gpu_submit_count_decode}</div>
          <div class="metric-label">GPU Submits (${m.gpu_submit_count_prefill} prefill + ${m.gpu_submit_count_decode} decode)</div>
        </div>
      </div>
`;

    // Latency chart
    if (result.raw?.decode_latencies_ms?.length > 0) {
      const latencies = result.raw.decode_latencies_ms;
      html += `
      <div class="chart-container">
        ${generateSVGLineChart(latencies, 600, 150, 'Decode Latency per Token', 'ms')}
      </div>
`;
    }

    // Percentiles table
    if (m.decode_ms_per_token_p50) {
      html += `
      <h3>Latency Percentiles</h3>
      <table>
        <tr><th>P50</th><th>P90</th><th>P99</th></tr>
        <tr>
          <td>${m.decode_ms_per_token_p50.toFixed(2)} ms</td>
          <td>${m.decode_ms_per_token_p90.toFixed(2)} ms</td>
          <td>${m.decode_ms_per_token_p99.toFixed(2)} ms</td>
        </tr>
      </table>
`;
    }

    html += `</div>`;
  }

  // Comparison with baseline
  if (baseline) {
    const comparisons = compareResults(baseline, firstResult);
    html += `
    <div class="card">
      <h2>Comparison vs Baseline</h2>
      <table>
        <tr><th>Metric</th><th>Baseline</th><th>Current</th><th>Change</th><th>Status</th></tr>
`;
    for (const c of comparisons) {
      const statusClass = c.improved ? 'better' : c.delta === 0 ? '' : 'worse';
      const sign = c.delta >= 0 ? '+' : '';
      html += `
        <tr>
          <td>${c.metric}</td>
          <td>${c.baseline.toFixed(2)}</td>
          <td>${c.current.toFixed(2)}</td>
          <td class="${statusClass}">${sign}${c.deltaPercent.toFixed(1)}%</td>
          <td class="${statusClass}">${c.improved ? 'Better' : c.delta === 0 ? 'Same' : 'Worse'}</td>
        </tr>
`;
    }
    html += `</table>`;

    // Bar chart comparison
    const chartData = comparisons.slice(0, 6).map((c) => [
      { label: `${c.metric} (base)`, value: c.baseline, color: '#94a3b8' },
      { label: `${c.metric} (curr)`, value: c.current, color: c.improved ? '#22c55e' : '#ef4444' },
    ]).flat();

    html += `
      <div class="chart-container">
        ${generateSVGBarChart(chartData.slice(0, 8), 700, 250, 'Baseline vs Current')}
      </div>
    </div>
`;
  }

  // Full metrics table
  html += `
    <div class="card">
      <h2>All Metrics</h2>
      <table>
        <tr><th>Metric</th><th>Value</th></tr>
`;
  const m = firstResult.metrics;
  const allMetrics = [
    ['TTFT', `${m.ttft_ms} ms`],
    ['Prefill Time', `${m.prefill_ms} ms`],
    ['Prefill Throughput', `${m.prefill_tokens_per_sec} tok/s`],
    ['Decode Time', `${m.decode_ms_total} ms`],
    ['Decode Throughput', `${m.decode_tokens_per_sec} tok/s`],
    ['GPU Submits (Prefill)', m.gpu_submit_count_prefill],
    ['GPU Submits (Decode)', m.gpu_submit_count_decode],
    ['GPU Readback Bytes', m.gpu_readback_bytes_total ? `${(m.gpu_readback_bytes_total / 1024).toFixed(1)} KB` : 'N/A'],
    ['Peak VRAM', m.estimated_vram_bytes_peak ? `${(m.estimated_vram_bytes_peak / 1024 / 1024).toFixed(1)} MB` : 'N/A'],
    ['GPU Timestamp Available', m.gpu_timestamp_available ? 'Yes' : 'No'],
  ];

  for (const [name, value] of allMetrics) {
    html += `<tr><td>${name}</td><td>${value}</td></tr>`;
  }

  html += `
      </table>
    </div>
    <div class="card">
      <p class="timestamp">Report generated by DOPPLER Benchmark CLI</p>
    </div>
  </div>
</body>
</html>`;

  return html;
}

// ============================================================================
// Output Formatting
// ============================================================================

function formatResults(results: any): void {
  if (Array.isArray(results)) {
    // Full suite results
    console.log('\n' + '='.repeat(60));
    console.log('BENCHMARK RESULTS');
    console.log('='.repeat(60));

    for (const result of results) {
      formatSingleResult(result);
    }
  } else if (results.suite === 'system') {
    // System benchmark
    formatSystemResult(results);
  } else {
    // Single pipeline result
    formatSingleResult(results);
  }
}

function formatSingleResult(result: any): void {
  const m = result.metrics;
  const model = result.model?.modelName ?? result.model?.modelId ?? 'unknown';
  const prompt = result.workload?.promptName ?? 'unknown';

  console.log(`\n--- ${model} (${prompt}) ---`);
  console.log(`TTFT:           ${m.ttft_ms} ms`);
  console.log(`Prefill:        ${m.prefill_ms} ms (${m.prefill_tokens_per_sec} tok/s)`);
  console.log(`Decode:         ${m.decode_ms_total} ms (${m.decode_tokens_per_sec} tok/s)`);
  console.log(`GPU Submits:    ${m.gpu_submit_count_prefill} prefill, ${m.gpu_submit_count_decode} decode`);

  if (m.decode_ms_per_token_p50) {
    console.log(`Latency P50/90/99: ${m.decode_ms_per_token_p50}/${m.decode_ms_per_token_p90}/${m.decode_ms_per_token_p99} ms`);
  }

  if (m.estimated_vram_bytes_peak) {
    const vramMB = (m.estimated_vram_bytes_peak / 1024 / 1024).toFixed(1);
    console.log(`Peak VRAM:      ${vramMB} MB`);
  }

  if (m.gpu_timestamp_available) {
    console.log(`GPU Time:       ${m.gpu_time_ms_prefill} ms prefill, ${m.gpu_time_ms_decode} ms decode`);
  }
}

function formatSystemResult(result: any): void {
  const { storage, download, opfs } = result;

  console.log('\n' + '='.repeat(60));
  console.log('SYSTEM BENCHMARK RESULTS');
  console.log('='.repeat(60));

  console.log('\nStorage:');
  console.log(`  Mode:       ${storage.mode}`);
  console.log(`  Persisted:  ${storage.persisted}`);
  console.log(`  Quota:      ${formatBytes(storage.quotaBytes)}`);
  console.log(`  Used:       ${formatBytes(storage.usageBytes)}`);

  console.log('\nDownload:');
  console.log(`  Shards:     ${download.shardCount}`);
  console.log(`  Total:      ${formatBytes(download.totalBytes)}`);
  console.log(`  Time:       ${download.totalTimeMs.toFixed(0)} ms`);
  console.log(`  Speed:      ${formatBytes(download.bytesPerSec)}/s`);

  if (opfs.available) {
    console.log('\nOPFS:');
    console.log(`  Write:      ${formatBytes(opfs.writeBytesPerSec)}/s`);
    console.log(`  Read:       ${formatBytes(opfs.readBytesPerSec)}/s`);
  } else {
    console.log('\nOPFS: not available');
  }
}

function formatBytes(bytes: number): string {
  if (bytes >= 1024 * 1024 * 1024) return `${(bytes / 1024 / 1024 / 1024).toFixed(2)} GB`;
  if (bytes >= 1024 * 1024) return `${(bytes / 1024 / 1024).toFixed(2)} MB`;
  if (bytes >= 1024) return `${(bytes / 1024).toFixed(2)} KB`;
  return `${bytes} bytes`;
}

// ============================================================================
// Main
// ============================================================================

async function main(): Promise<void> {
  const opts = parseArgs(process.argv.slice(2));

  if (opts.help) {
    printHelp();
    process.exit(0);
  }

  // Track memory usage (Node.js heap)
  const memBefore = process.memoryUsage();

  try {
    const results = await runBenchmarkInBrowser(opts);

    // Track memory after
    const memAfter = process.memoryUsage();
    const heapDelta = memAfter.heapUsed - memBefore.heapUsed;
    if (Math.abs(heapDelta) > 50 * 1024 * 1024) {
      // >50MB heap growth
      console.warn(`\nWarning: Significant heap growth detected (${(heapDelta / 1024 / 1024).toFixed(1)}MB)`);
    }

    // Format and print results
    formatResults(results);

    // Compare against baseline if provided
    let baseline: any = null;
    if (opts.compare) {
      try {
        const baselinePath = resolve(opts.compare);
        const baselineJson = await readFile(baselinePath, 'utf-8');
        baseline = JSON.parse(baselineJson);
        const comparison = compareResults(baseline, results);
        console.log(formatComparison(comparison));

        // Run t-test on decode latencies if available
        const baseLatencies = baseline.raw?.decode_latencies_ms;
        const currLatencies = results.raw?.decode_latencies_ms;
        if (baseLatencies?.length >= 3 && currLatencies?.length >= 3) {
          console.log('\n' + '─'.repeat(60));
          console.log('STATISTICAL SIGNIFICANCE (Welch\'s t-test)');
          console.log('─'.repeat(60));
          const ttest = welchTTest(baseLatencies, currLatencies);
          console.log(formatTTestResult('Decode Latency', ttest));
          if (ttest.significant) {
            console.log(`  -> The difference IS statistically significant (p < 0.05)`);
          } else {
            console.log(`  -> The difference is NOT statistically significant (p >= 0.05)`);
          }
        }
      } catch (err) {
        console.error(`\nFailed to load baseline for comparison: ${(err as Error).message}`);
      }
    }

    // Auto-save to tests/results/ with proper naming for comparison
    const resultsDir = resolve(__dirname, '../tests/results');
    await mkdir(resultsDir, { recursive: true });

    const autoFilename = generateResultFilename(results);
    const autoPath = resolve(resultsDir, autoFilename);
    await writeFile(autoPath, JSON.stringify(results, null, 2));
    console.log(`\nResults auto-saved to: ${autoPath}`);

    // Always generate HTML report (to tests/results/ by default, or custom path)
    const htmlFilename = autoFilename.replace('.json', '.html');
    const htmlPath = opts.html ? resolve(opts.html) : resolve(resultsDir, htmlFilename);
    await mkdir(dirname(htmlPath), { recursive: true });
    const htmlContent = generateHTMLReport(results, baseline);
    await writeFile(htmlPath, htmlContent);
    console.log(`HTML report saved to: ${htmlPath}`);

    // Also save to custom path if requested
    if (opts.output) {
      const outputPath = resolve(opts.output);
      await mkdir(dirname(outputPath), { recursive: true });
      await writeFile(outputPath, JSON.stringify(results, null, 2));
      console.log(`Results also saved to: ${outputPath}`);
    }

    // Print JSON to stdout for piping (unless --quiet)
    if (!opts.quiet) {
      console.log('\n' + '─'.repeat(60));
      console.log('JSON Output:');
      console.log('─'.repeat(60));
      console.log(JSON.stringify(results, null, 2));
    }

    process.exit(0);
  } catch (err) {
    console.error('\nBenchmark failed:', (err as Error).message);
    process.exit(1);
  }
}

main();

# DOPPLER RDRR Format Specification

Defines the Recursive DOPPLER Runtime Registry (RDRR) format for streaming model delivery optimized for browser-based LLM inference.

See also: `docs/ARCHITECTURE.md` for system overview.

---

## Goals

- Enable streaming download and incremental model loading in browsers
- Support P2P distribution with per-shard integrity verification
- Provide browser-native storage via OPFS (Origin Private File System)
- Support multiple quantization formats (Q4_K_M, F16, F32)

---

## Core Concepts

### RDRR

**RDRR** = **R**ecursive **D**OPPLER **R**untime **R**egistry

A streaming model delivery format bridging REPLOID (agent sandbox) and DOPPLER (WebGPU runtime).

| Component | Description |
|-----------|-------------|
| **Recursive** | Self-referential structure supporting nested model components |
| **DOPPLER** | The DOPPLER inference engine (WebGPU runtime) |
| **Runtime** | Execution-ready quantized weights |
| **Registry** | Manifest-based tensor addressing and shard management |

### Sharding

Models are split into fixed-size shards (default 64MB) for streaming download and P2P distribution. Each shard is independently verifiable.

### Manifest

A JSON file describing model metadata, shard layout, and tensor locations within shards.

---

## File Structure

```
model-name-rdrr/
├── manifest.json       # Model metadata and shard layout
├── shard_00000.bin     # 64MB shard (aligned to 4KB)
├── shard_00001.bin
├── shard_00002.bin
└── ...
```

---

## Manifest Schema

### Required Fields

```json
{
  "version": "1.0",
  "modelId": "sha256-hash-of-model",
  "modelType": "transformer",
  "architecture": "Llama3ForCausalLM",
  "quantization": "Q4_K_M",
  "hashAlgorithm": "sha256",
  "config": {
    "hidden_size": 4096,
    "num_hidden_layers": 32,
    "num_attention_heads": 32,
    "vocab_size": 128256
  },
  "shards": [...],
  "tensors": {...},
  "totalSize": 3400000000,
  "tensorCount": 340
}
```

### Shard Entry

```json
{
  "index": 0,
  "fileName": "shard_00000.bin",
  "size": 67108864,
  "hash": "sha256-hex-64-chars",
  "hashAlgorithm": "sha256"
}
```

### Tensor Entry

```json
{
  "model.embed_tokens.weight": {
    "shard": 0,
    "offset": 0,
    "size": 123456789,
    "shape": [128256, 4096],
    "dtype": "Q4_K_M"
  }
}
```

### Multi-Shard Tensors

For tensors spanning multiple shards, use the `spans` field:

```json
{
  "model.embed_tokens.weight": {
    "shard": 0,
    "offset": 0,
    "size": 123456789,
    "shape": [128256, 4096],
    "dtype": "Q4_K_M",
    "spans": [
      { "shardIndex": 0, "offset": 0, "size": 67108864 },
      { "shardIndex": 1, "offset": 0, "size": 56347925 }
    ]
  }
}
```

### Optional Fields

| Field | Type | Description |
|-------|------|-------------|
| `tokenizer` | object | Tokenizer configuration |
| `moeConfig` | object | Mixture-of-experts configuration |
| `runtimeOptimizations` | object | Hints for kernel selection |
| `blake3Full` | string | Full-model BLAKE3 hash |

---

## Design Principles

### Sharded for Streaming

- 64MB default shard size (configurable)
- 4KB alignment for optimal OPFS/disk I/O
- Supports streaming download and incremental loading

### Integrity Verification

- Per-shard hash (SHA-256 supported everywhere, BLAKE3 optional)
- Enables P2P distribution without trusting the source
- Peers can verify shard integrity independently

### Browser-Native

- Stored in OPFS (Origin Private File System)
- Compatible with WebGPU tensor loading
- No WASM file system emulation needed

### Quantization Support

| Format | Description |
|--------|-------------|
| Q4_K_M | 4-bit GGML k-quants |
| F16 | Half precision |
| F32 | Full precision |

---

## Field Normalization

The on-disk `manifest.json` may vary in naming. At runtime, `storage/rdrr-format.ts` normalizes:

| On-Disk | Normalized |
|---------|------------|
| `fileName` or `filename` | `filename` |
| `hash` or `blake3` | `hash` (with `blake3` alias) |
| missing `offset` | computed from previous shards |
| `hashAlgorithm` | inferred from manifest or shard entry |

This ensures compatibility across converter versions.

---

## Usage

### Converting Models

```bash
# From GGUF
npx tsx tools/convert-cli.ts model.gguf ./output-rdrr

# From Safetensors (HuggingFace format)
npx tsx tools/convert-cli.ts ./hf-model-dir ./output-rdrr --quantize q4_k_m
```

### Serving Models

```bash
# Serve converted model
npx tsx tools/serve-cli.ts ./model-rdrr --port 8765

# Convert and serve in one step
npx tsx tools/serve-cli.ts model.gguf
```

### Loading in Browser

```javascript
import { downloadModel, parseManifest, createPipeline } from 'doppler';

// Download model to OPFS
await downloadModel('http://localhost:8765');

// Load and create inference pipeline
const manifest = await loadManifestFromOPFS();
const pipeline = await createPipeline(parseManifest(manifest));

// Generate
for await (const token of pipeline.generate('Hello')) {
  console.log(token);
}
```

---

## Version History

| Version | Changes |
|---------|---------|
| 1.0 | Initial release with Q4_K_M, F16, F32 support |

---

## Related Files

- `storage/rdrr-format.ts`: Parser and validation
- `tools/rdrr-writer.ts`: Writer for conversion
- `storage/shard-manager.ts`: OPFS shard management
- `storage/downloader.ts`: Resumable downloads

---

*Last updated: December 2025*
